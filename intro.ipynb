{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:f32[] = log a\n",
       "    c:f32[] = log 2.0\n",
       "    d:f32[] = div b c\n",
       "  in (d,) }"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    ln_x = jnp.log(x)\n",
    "    ln_2 = jnp.log(2)\n",
    "    return ln_x/ln_2\n",
    "\n",
    "from jax import make_jaxpr\n",
    "\n",
    "x_0 = 2.0\n",
    "\n",
    "make_jaxpr(f)(x_0)  # let's watch JAX trace the function! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:f32[] = log a\n",
       "    c:f32[] = log 2.0\n",
       "    _:f32[] = div b c\n",
       "    d:f32[] = div 1.0 c\n",
       "    e:f32[] = div d a\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = jax.grad(f)\n",
    "make_jaxpr(gradient)(x_0)  # this is the JAX expression for the gradient\n",
    "# note how the original expression is embedded in the gradient expression!\n",
    "# this is JAX tracing the computation graph and then using the primitive rules\n",
    "# for log and division to compute the gradient (d/dx log(x) = 1/x, d/dx x/y = 1/y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the step-by-step representation of the given expression using LaTeX equations:\n",
    "\n",
    "Step 1:\n",
    "$ b = \\log(a) $\n",
    "\n",
    "Step 2:\n",
    "$ c = \\log(2.0) $\n",
    "\n",
    "Step 3:\n",
    "$ \\_ = \\frac{b}{c} $\n",
    "\n",
    "Step 4:\n",
    "$ d = \\frac{1.0}{c} = \\frac{1.0}{\\log(2.0)} $, which is $\\frac{\\partial \\_}{\\partial b}$\n",
    "\n",
    "Step 5:\n",
    "$ e = \\frac{d}{a} $, which is $\\frac{\\partial \\_}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a}$ ,\n",
    "\n",
    "since $\\frac{\\partial b}{\\partial a} = \\frac{\\partial \\log(a)}{\\partial a} = \\frac{1}{a}$\n",
    "\n",
    "Step 6:\n",
    " return $e $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.7213475, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.7213475, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (x_0 * jnp.log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:f32[] = log a\n",
       "    c:f32[] = log 2.0\n",
       "    _:f32[] = div b c\n",
       "    d:f32[] = div 1.0 c\n",
       "    _:f32[] = div d a\n",
       "    e:f32[] = integer_pow[y=-2] a\n",
       "    f:f32[] = mul 1.0 e\n",
       "    g:f32[] = mul f d\n",
       "    h:f32[] = neg g\n",
       "  in (h,) }"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's the gradient of the gradient\n",
    "make_jaxpr(jax.grad(gradient))(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool, weak_type=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which is the derivative of 1/x = -1/x^2\n",
    "-1 / (x_0**2 * jnp.log(2)) == jax.grad(gradient)(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:f32[] = log a\n",
       "    c:f32[] = log 2.0\n",
       "    _:f32[] = div b c\n",
       "    d:f32[] = div 1.0 c\n",
       "    _:f32[] = div d a\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    e:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 e\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    f:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 f\n",
       "    g:f32[] = integer_pow[y=-3] a\n",
       "    h:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 h\n",
       "    _:f32[] = mul -2.0 g\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    i:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 i\n",
       "    j:f32[] = integer_pow[y=-3] a\n",
       "    k:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 k\n",
       "    _:f32[] = mul -2.0 j\n",
       "    l:f32[] = integer_pow[y=-3] a\n",
       "    m:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 m\n",
       "    n:f32[] = integer_pow[y=-4] a\n",
       "    o:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 o\n",
       "    _:f32[] = mul -3.0 n\n",
       "    _:f32[] = mul -2.0 l\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    p:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 p\n",
       "    q:f32[] = integer_pow[y=-3] a\n",
       "    r:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 r\n",
       "    _:f32[] = mul -2.0 q\n",
       "    s:f32[] = integer_pow[y=-3] a\n",
       "    t:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 t\n",
       "    u:f32[] = integer_pow[y=-4] a\n",
       "    v:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 v\n",
       "    _:f32[] = mul -3.0 u\n",
       "    _:f32[] = mul -2.0 s\n",
       "    w:f32[] = integer_pow[y=-3] a\n",
       "    x:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 x\n",
       "    y:f32[] = integer_pow[y=-4] a\n",
       "    z:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 z\n",
       "    _:f32[] = mul -3.0 y\n",
       "    ba:f32[] = integer_pow[y=-4] a\n",
       "    bb:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 bb\n",
       "    bc:f32[] = integer_pow[y=-5] a\n",
       "    bd:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 bd\n",
       "    _:f32[] = mul -4.0 bc\n",
       "    _:f32[] = mul -3.0 ba\n",
       "    _:f32[] = mul -2.0 w\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    be:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 be\n",
       "    bf:f32[] = integer_pow[y=-3] a\n",
       "    bg:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 bg\n",
       "    _:f32[] = mul -2.0 bf\n",
       "    bh:f32[] = integer_pow[y=-3] a\n",
       "    bi:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 bi\n",
       "    bj:f32[] = integer_pow[y=-4] a\n",
       "    bk:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 bk\n",
       "    _:f32[] = mul -3.0 bj\n",
       "    _:f32[] = mul -2.0 bh\n",
       "    bl:f32[] = integer_pow[y=-3] a\n",
       "    bm:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 bm\n",
       "    bn:f32[] = integer_pow[y=-4] a\n",
       "    bo:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 bo\n",
       "    _:f32[] = mul -3.0 bn\n",
       "    bp:f32[] = integer_pow[y=-4] a\n",
       "    bq:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 bq\n",
       "    br:f32[] = integer_pow[y=-5] a\n",
       "    bs:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 bs\n",
       "    _:f32[] = mul -4.0 br\n",
       "    _:f32[] = mul -3.0 bp\n",
       "    _:f32[] = mul -2.0 bl\n",
       "    bt:f32[] = integer_pow[y=-3] a\n",
       "    bu:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 bu\n",
       "    bv:f32[] = integer_pow[y=-4] a\n",
       "    bw:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 bw\n",
       "    _:f32[] = mul -3.0 bv\n",
       "    bx:f32[] = integer_pow[y=-4] a\n",
       "    by:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 by\n",
       "    bz:f32[] = integer_pow[y=-5] a\n",
       "    ca:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ca\n",
       "    _:f32[] = mul -4.0 bz\n",
       "    _:f32[] = mul -3.0 bx\n",
       "    cb:f32[] = integer_pow[y=-4] a\n",
       "    cc:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 cc\n",
       "    cd:f32[] = integer_pow[y=-5] a\n",
       "    ce:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ce\n",
       "    _:f32[] = mul -4.0 cd\n",
       "    cf:f32[] = integer_pow[y=-5] a\n",
       "    cg:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 cg\n",
       "    ch:f32[] = integer_pow[y=-6] a\n",
       "    ci:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 ci\n",
       "    _:f32[] = mul -5.0 ch\n",
       "    _:f32[] = mul -4.0 cf\n",
       "    _:f32[] = mul -3.0 cb\n",
       "    _:f32[] = mul -2.0 bt\n",
       "    cj:f32[] = integer_pow[y=-2] a\n",
       "    ck:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 ck\n",
       "    cl:f32[] = integer_pow[y=-3] a\n",
       "    cm:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 cm\n",
       "    _:f32[] = mul -2.0 cl\n",
       "    cn:f32[] = integer_pow[y=-3] a\n",
       "    co:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 co\n",
       "    cp:f32[] = integer_pow[y=-4] a\n",
       "    cq:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 cq\n",
       "    _:f32[] = mul -3.0 cp\n",
       "    _:f32[] = mul -2.0 cn\n",
       "    cr:f32[] = integer_pow[y=-3] a\n",
       "    cs:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 cs\n",
       "    ct:f32[] = integer_pow[y=-4] a\n",
       "    cu:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 cu\n",
       "    _:f32[] = mul -3.0 ct\n",
       "    cv:f32[] = integer_pow[y=-4] a\n",
       "    cw:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 cw\n",
       "    cx:f32[] = integer_pow[y=-5] a\n",
       "    cy:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 cy\n",
       "    _:f32[] = mul -4.0 cx\n",
       "    _:f32[] = mul -3.0 cv\n",
       "    _:f32[] = mul -2.0 cr\n",
       "    cz:f32[] = integer_pow[y=-3] a\n",
       "    da:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 da\n",
       "    db:f32[] = integer_pow[y=-4] a\n",
       "    dc:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 dc\n",
       "    _:f32[] = mul -3.0 db\n",
       "    dd:f32[] = integer_pow[y=-4] a\n",
       "    de:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 de\n",
       "    df:f32[] = integer_pow[y=-5] a\n",
       "    dg:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 dg\n",
       "    _:f32[] = mul -4.0 df\n",
       "    _:f32[] = mul -3.0 dd\n",
       "    dh:f32[] = integer_pow[y=-4] a\n",
       "    di:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 di\n",
       "    dj:f32[] = integer_pow[y=-5] a\n",
       "    dk:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 dk\n",
       "    _:f32[] = mul -4.0 dj\n",
       "    dl:f32[] = integer_pow[y=-5] a\n",
       "    dm:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 dm\n",
       "    dn:f32[] = integer_pow[y=-6] a\n",
       "    do:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 do\n",
       "    _:f32[] = mul -5.0 dn\n",
       "    _:f32[] = mul -4.0 dl\n",
       "    _:f32[] = mul -3.0 dh\n",
       "    _:f32[] = mul -2.0 cz\n",
       "    dp:f32[] = integer_pow[y=-3] a\n",
       "    dq:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 dq\n",
       "    dr:f32[] = integer_pow[y=-4] a\n",
       "    ds:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 ds\n",
       "    _:f32[] = mul -3.0 dr\n",
       "    dt:f32[] = integer_pow[y=-4] a\n",
       "    du:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 du\n",
       "    dv:f32[] = integer_pow[y=-5] a\n",
       "    dw:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 dw\n",
       "    _:f32[] = mul -4.0 dv\n",
       "    _:f32[] = mul -3.0 dt\n",
       "    dx:f32[] = integer_pow[y=-4] a\n",
       "    dy:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 dy\n",
       "    dz:f32[] = integer_pow[y=-5] a\n",
       "    ea:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ea\n",
       "    _:f32[] = mul -4.0 dz\n",
       "    eb:f32[] = integer_pow[y=-5] a\n",
       "    ec:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ec\n",
       "    ed:f32[] = integer_pow[y=-6] a\n",
       "    ee:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 ee\n",
       "    _:f32[] = mul -5.0 ed\n",
       "    _:f32[] = mul -4.0 eb\n",
       "    _:f32[] = mul -3.0 dx\n",
       "    ef:f32[] = integer_pow[y=-4] a\n",
       "    eg:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 eg\n",
       "    eh:f32[] = integer_pow[y=-5] a\n",
       "    ei:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ei\n",
       "    _:f32[] = mul -4.0 eh\n",
       "    ej:f32[] = integer_pow[y=-5] a\n",
       "    ek:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ek\n",
       "    el:f32[] = integer_pow[y=-6] a\n",
       "    em:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 em\n",
       "    _:f32[] = mul -5.0 el\n",
       "    _:f32[] = mul -4.0 ej\n",
       "    en:f32[] = integer_pow[y=-5] a\n",
       "    eo:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 eo\n",
       "    ep:f32[] = integer_pow[y=-6] a\n",
       "    eq:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 eq\n",
       "    _:f32[] = mul -5.0 ep\n",
       "    er:f32[] = integer_pow[y=-6] a\n",
       "    es:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 es\n",
       "    et:f32[] = integer_pow[y=-7] a\n",
       "    eu:f32[] = integer_pow[y=-8] a\n",
       "    ev:f32[] = mul -7.0 eu\n",
       "    ew:f32[] = mul -6.0 et\n",
       "    ex:f32[] = mul -5.0 er\n",
       "    ey:f32[] = mul -4.0 en\n",
       "    ez:f32[] = mul -3.0 ef\n",
       "    fa:f32[] = mul -2.0 dp\n",
       "    fb:f32[] = mul 1.0 cj\n",
       "    fc:f32[] = mul fb d\n",
       "    _:f32[] = neg fc\n",
       "    fd:f32[] = neg 1.0\n",
       "    fe:f32[] = mul fd d\n",
       "    ff:f32[] = mul 1.0 fe\n",
       "    _:f32[] = mul ff fa\n",
       "    fg:f32[] = mul ff 1.0\n",
       "    fh:f32[] = mul -2.0 fg\n",
       "    _:f32[] = mul fh ez\n",
       "    fi:f32[] = mul fh 1.0\n",
       "    fj:f32[] = mul -3.0 fi\n",
       "    _:f32[] = mul fj ey\n",
       "    fk:f32[] = mul fj 1.0\n",
       "    fl:f32[] = mul -4.0 fk\n",
       "    _:f32[] = mul fl ex\n",
       "    fm:f32[] = mul fl 1.0\n",
       "    fn:f32[] = mul -5.0 fm\n",
       "    _:f32[] = mul fn ew\n",
       "    fo:f32[] = mul fn 1.0\n",
       "    fp:f32[] = mul -6.0 fo\n",
       "    fq:f32[] = mul fp ev\n",
       "  in (fq,) }"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could go forever:\n",
    "make_jaxpr(jax.grad(jax.grad(jax.grad(jax.grad(jax.grad(jax.grad(jax.grad(jax.grad(f)))))))))(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[] b:f32[] c:f32[]. let\n",
       "    d:f32[] = mul b a\n",
       "    e:f32[] = add d c\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's another function: a layer of a neural network\n",
    "def mlp(x, w, b):\n",
    "    return jnp.dot(w, x) + b\n",
    "\n",
    "x = 9.0\n",
    "w = 3.0\n",
    "b = 1.0\n",
    "\n",
    "make_jaxpr(mlp)(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[] b:f32[] c:f32[]. let\n",
       "    d:f32[] = mul b a\n",
       "    _:f32[] = add d c\n",
       "    e:f32[] = mul 1.0 a\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the gradient of f with respect to w\n",
    "make_jaxpr(jax.grad(mlp, argnums=1))(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool, weak_type=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analytical gradient is just x\n",
    "x == jax.grad(mlp, argnums=1)(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[3,3] b:f32[3,3] c:f32[3,3] d:f32[3]. let\n",
       "    e:f32[3] = dot_general[dimension_numbers=(([1], [0]), ([], []))] b d\n",
       "    f:f32[3] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a d\n",
       "    g:f32[3] = dot_general[dimension_numbers=(([1], [0]), ([], []))] c d\n",
       "    h:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] e f\n",
       "    i:f32[] = sqrt 3.0\n",
       "    j:f32[] = convert_element_type[new_dtype=float32 weak_type=False] i\n",
       "    k:f32[] = div h j\n",
       "    l:f32[] = exp k\n",
       "    m:f32[] = exp k\n",
       "    n:f32[] = reduce_sum[axes=()] m\n",
       "    o:f32[] = div l n\n",
       "    p:f32[3] = mul o g\n",
       "  in (p,) }"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's get crazy: here's the attention mechanism from the Transformer\n",
    "# (https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "def softmax(x):\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v):\n",
    "    qk = jnp.dot(q, k.T) / jnp.sqrt(q.shape[-1])\n",
    "    weights = softmax(qk)\n",
    "    return jnp.dot(weights, v)\n",
    "\n",
    "def attention(weights, e):\n",
    "    Wq = weights[\"Wq\"]\n",
    "    Wk = weights[\"Wk\"]\n",
    "    Wv = weights[\"Wv\"]\n",
    "\n",
    "    q = jnp.dot(Wq, e)\n",
    "    k = jnp.dot(Wk, e)\n",
    "    v = jnp.dot(Wv, e)\n",
    "    return scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "Wq = jnp.array([[1.0, 0.1, 0.0],\n",
    "                [0.0, 1.0, 0.1],\n",
    "                [0.1, 0.0, 1.0]])\n",
    "\n",
    "Wk = jnp.array([[1.0, 0.0, 0.1],\n",
    "                [0.1, 1.0, 0.0],\n",
    "                [0.0, 0.1, 1.0]])\n",
    "\n",
    "Wv = jnp.array([[1.0, 0.1, 0.0],\n",
    "                [0.0, 1.0, 0.1],\n",
    "                [0.1, 0.0, 1.0]])\n",
    "\n",
    "\n",
    "e = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "make_jaxpr(attention)(dict(Wq=Wq, Wk=Wk, Wv=Wv), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1.2, 2.3, 3.1], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(dict(Wq=Wq, Wk=Wk, Wv=Wv), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Wk': Array([[[-8.2590617e-08, -1.6518123e-07, -2.4777185e-07],\n",
       "         [-1.5829868e-07, -3.1659735e-07, -4.7489601e-07],\n",
       "         [-2.1335909e-07, -4.2671817e-07, -6.4007725e-07]],\n",
       " \n",
       "        [[-1.6518123e-07, -3.3036247e-07, -4.9554370e-07],\n",
       "         [-3.1659735e-07, -6.3319470e-07, -9.4979202e-07],\n",
       "         [-4.2671817e-07, -8.5343635e-07, -1.2801545e-06]],\n",
       " \n",
       "        [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]]], dtype=float32),\n",
       " 'Wq': Array([[[-8.9473168e-08, -1.7894634e-07, -2.6841951e-07],\n",
       "         [-1.4453357e-07, -2.8906715e-07, -4.3360072e-07],\n",
       "         [-2.2024165e-07, -4.4048329e-07, -6.6072494e-07]],\n",
       " \n",
       "        [[-1.7894634e-07, -3.5789267e-07, -5.3683902e-07],\n",
       "         [-2.8906715e-07, -5.7813429e-07, -8.6720144e-07],\n",
       "         [-4.4048329e-07, -8.8096658e-07, -1.3214499e-06]],\n",
       " \n",
       "        [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]]], dtype=float32),\n",
       " 'Wv': Array([[[1., 2., 3.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0.],\n",
       "         [1., 2., 3.],\n",
       "         [0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 2., 3.]]], dtype=float32)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the gradient of the attention mechanism with respect to the weights\n",
    "# note that we have to use the full jacobian, since the output of our function\n",
    "# is a vector, not a scalar\n",
    "# the result is a 3x3 matrix, where each row is the gradient of the output\n",
    "# with respect to the corresponding row of the weight matrix\n",
    "# note how simple the Wv gradient is, since all we do is a dot product\n",
    "jax.jacobian(attention, argnums=0)(dict(Wq=Wq, Wk=Wk, Wv=Wv), e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vmap (automatic vectorization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have you ever written a very simple function that you wanted to apply to a large array of inputs? numpy makes this easy with vectorization, but jax takes it a step further with vmap.\n",
    "\n",
    "vmap is a function transformation that allows you to take a function that operates on a single input, and transform it into a function that operates on many inputs in a vectorized manner.\n",
    "\n",
    "for example, consider the following function that would be a litte tricky to vectorize with numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, inputs):\n",
    "  for W, b in params:\n",
    "    outputs = jnp.dot(inputs, W) + b\n",
    "    inputs = jnp.tanh(outputs)\n",
    "  return outputs\n",
    "\n",
    "params = [\n",
    "    (jnp.array([[1., 2.], [3., 4.]]), jnp.array([1., 1.])),\n",
    "]\n",
    "\n",
    "inputs = jnp.array([[1., 2.]])\n",
    "predict(params, inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we can use vmap to vectorize this function over multiple inputs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 8., 11.],\n",
       "       [16., 23.],\n",
       "       [24., 35.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_predict = jax.vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "many_inputs = jnp.array([[1., 2.], [3., 4.], [5., 6.]])\n",
    "batched_predict(params, many_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with literally zero brain cells, we have implemented batching! :)\n",
    "\n",
    "the `in_axes` argument specifies which of the function’s arguments to batch over. in this case, we want to batch over the second argument, which is the inputs. the first argument, params, will be left unchanged -- it will be as if we called predict in a for loop over each of the inputs.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's another one of my favorite examples -- operating over pairs of inputs using nested vmaps:\n",
    "\n",
    "```python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
