{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:f32[] = log a\n",
       "    c:f32[] = log 2.0\n",
       "    d:f32[] = div b c\n",
       "  in (d,) }"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    ln_x = jnp.log(x)\n",
    "    ln_2 = jnp.log(2)\n",
    "    return ln_x/ln_2\n",
    "\n",
    "from jax import make_jaxpr\n",
    "\n",
    "x_0 = 2.0\n",
    "\n",
    "make_jaxpr(f)(x_0)  # let's watch JAX trace the function! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:f32[] = log a\n",
       "    c:f32[] = log 2.0\n",
       "    _:f32[] = div b c\n",
       "    d:f32[] = div 1.0 c\n",
       "    e:f32[] = div d a\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = jax.grad(f)\n",
    "make_jaxpr(gradient)(x_0)  # this is the JAX expression for the gradient\n",
    "# note how the original expression is embedded in the gradient expression!\n",
    "# this is JAX tracing the computation graph and then using the primitive rules\n",
    "# for log and division to compute the gradient (d/dx log(x) = 1/x, d/dx x/y = 1/y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the step-by-step representation of the given expression using LaTeX equations:\n",
    "\n",
    "Step 1:\n",
    "$ b = \\log(a) $\n",
    "\n",
    "Step 2:\n",
    "$ c = \\log(2.0) $\n",
    "\n",
    "Step 3:\n",
    "$ \\_ = \\frac{b}{c} $\n",
    "\n",
    "Step 4:\n",
    "$ d = \\frac{1.0}{c} = \\frac{1.0}{\\log(2.0)} $, which is $\\frac{\\partial \\_}{\\partial b}$\n",
    "\n",
    "Step 5:\n",
    "$ e = \\frac{d}{a} $, which is $\\frac{\\partial \\_}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a}$ ,\n",
    "\n",
    "since $\\frac{\\partial b}{\\partial a} = \\frac{\\partial \\log(a)}{\\partial a} = \\frac{1}{a}$\n",
    "\n",
    "Step 6:\n",
    " return $e $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.7213475, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.7213475, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (x_0 * jnp.log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:f32[] = log a\n",
       "    c:f32[] = log 2.0\n",
       "    _:f32[] = div b c\n",
       "    d:f32[] = div 1.0 c\n",
       "    _:f32[] = div d a\n",
       "    e:f32[] = integer_pow[y=-2] a\n",
       "    f:f32[] = mul 1.0 e\n",
       "    g:f32[] = mul f d\n",
       "    h:f32[] = neg g\n",
       "  in (h,) }"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's the gradient of the gradient\n",
    "make_jaxpr(jax.grad(gradient))(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool, weak_type=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which is the derivative of 1/x = -1/x^2\n",
    "-1 / (x_0**2 * jnp.log(2)) == jax.grad(gradient)(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:f32[] = log a\n",
       "    c:f32[] = log 2.0\n",
       "    _:f32[] = div b c\n",
       "    d:f32[] = div 1.0 c\n",
       "    _:f32[] = div d a\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    e:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 e\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    f:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 f\n",
       "    g:f32[] = integer_pow[y=-3] a\n",
       "    h:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 h\n",
       "    _:f32[] = mul -2.0 g\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    i:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 i\n",
       "    j:f32[] = integer_pow[y=-3] a\n",
       "    k:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 k\n",
       "    _:f32[] = mul -2.0 j\n",
       "    l:f32[] = integer_pow[y=-3] a\n",
       "    m:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 m\n",
       "    n:f32[] = integer_pow[y=-4] a\n",
       "    o:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 o\n",
       "    _:f32[] = mul -3.0 n\n",
       "    _:f32[] = mul -2.0 l\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    p:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 p\n",
       "    q:f32[] = integer_pow[y=-3] a\n",
       "    r:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 r\n",
       "    _:f32[] = mul -2.0 q\n",
       "    s:f32[] = integer_pow[y=-3] a\n",
       "    t:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 t\n",
       "    u:f32[] = integer_pow[y=-4] a\n",
       "    v:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 v\n",
       "    _:f32[] = mul -3.0 u\n",
       "    _:f32[] = mul -2.0 s\n",
       "    w:f32[] = integer_pow[y=-3] a\n",
       "    x:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 x\n",
       "    y:f32[] = integer_pow[y=-4] a\n",
       "    z:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 z\n",
       "    _:f32[] = mul -3.0 y\n",
       "    ba:f32[] = integer_pow[y=-4] a\n",
       "    bb:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 bb\n",
       "    bc:f32[] = integer_pow[y=-5] a\n",
       "    bd:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 bd\n",
       "    _:f32[] = mul -4.0 bc\n",
       "    _:f32[] = mul -3.0 ba\n",
       "    _:f32[] = mul -2.0 w\n",
       "    _:f32[] = integer_pow[y=-2] a\n",
       "    be:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 be\n",
       "    bf:f32[] = integer_pow[y=-3] a\n",
       "    bg:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 bg\n",
       "    _:f32[] = mul -2.0 bf\n",
       "    bh:f32[] = integer_pow[y=-3] a\n",
       "    bi:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 bi\n",
       "    bj:f32[] = integer_pow[y=-4] a\n",
       "    bk:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 bk\n",
       "    _:f32[] = mul -3.0 bj\n",
       "    _:f32[] = mul -2.0 bh\n",
       "    bl:f32[] = integer_pow[y=-3] a\n",
       "    bm:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 bm\n",
       "    bn:f32[] = integer_pow[y=-4] a\n",
       "    bo:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 bo\n",
       "    _:f32[] = mul -3.0 bn\n",
       "    bp:f32[] = integer_pow[y=-4] a\n",
       "    bq:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 bq\n",
       "    br:f32[] = integer_pow[y=-5] a\n",
       "    bs:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 bs\n",
       "    _:f32[] = mul -4.0 br\n",
       "    _:f32[] = mul -3.0 bp\n",
       "    _:f32[] = mul -2.0 bl\n",
       "    bt:f32[] = integer_pow[y=-3] a\n",
       "    bu:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 bu\n",
       "    bv:f32[] = integer_pow[y=-4] a\n",
       "    bw:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 bw\n",
       "    _:f32[] = mul -3.0 bv\n",
       "    bx:f32[] = integer_pow[y=-4] a\n",
       "    by:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 by\n",
       "    bz:f32[] = integer_pow[y=-5] a\n",
       "    ca:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ca\n",
       "    _:f32[] = mul -4.0 bz\n",
       "    _:f32[] = mul -3.0 bx\n",
       "    cb:f32[] = integer_pow[y=-4] a\n",
       "    cc:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 cc\n",
       "    cd:f32[] = integer_pow[y=-5] a\n",
       "    ce:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ce\n",
       "    _:f32[] = mul -4.0 cd\n",
       "    cf:f32[] = integer_pow[y=-5] a\n",
       "    cg:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 cg\n",
       "    ch:f32[] = integer_pow[y=-6] a\n",
       "    ci:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 ci\n",
       "    _:f32[] = mul -5.0 ch\n",
       "    _:f32[] = mul -4.0 cf\n",
       "    _:f32[] = mul -3.0 cb\n",
       "    _:f32[] = mul -2.0 bt\n",
       "    cj:f32[] = integer_pow[y=-2] a\n",
       "    ck:f32[] = integer_pow[y=-3] a\n",
       "    _:f32[] = mul -2.0 ck\n",
       "    cl:f32[] = integer_pow[y=-3] a\n",
       "    cm:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 cm\n",
       "    _:f32[] = mul -2.0 cl\n",
       "    cn:f32[] = integer_pow[y=-3] a\n",
       "    co:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 co\n",
       "    cp:f32[] = integer_pow[y=-4] a\n",
       "    cq:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 cq\n",
       "    _:f32[] = mul -3.0 cp\n",
       "    _:f32[] = mul -2.0 cn\n",
       "    cr:f32[] = integer_pow[y=-3] a\n",
       "    cs:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 cs\n",
       "    ct:f32[] = integer_pow[y=-4] a\n",
       "    cu:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 cu\n",
       "    _:f32[] = mul -3.0 ct\n",
       "    cv:f32[] = integer_pow[y=-4] a\n",
       "    cw:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 cw\n",
       "    cx:f32[] = integer_pow[y=-5] a\n",
       "    cy:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 cy\n",
       "    _:f32[] = mul -4.0 cx\n",
       "    _:f32[] = mul -3.0 cv\n",
       "    _:f32[] = mul -2.0 cr\n",
       "    cz:f32[] = integer_pow[y=-3] a\n",
       "    da:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 da\n",
       "    db:f32[] = integer_pow[y=-4] a\n",
       "    dc:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 dc\n",
       "    _:f32[] = mul -3.0 db\n",
       "    dd:f32[] = integer_pow[y=-4] a\n",
       "    de:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 de\n",
       "    df:f32[] = integer_pow[y=-5] a\n",
       "    dg:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 dg\n",
       "    _:f32[] = mul -4.0 df\n",
       "    _:f32[] = mul -3.0 dd\n",
       "    dh:f32[] = integer_pow[y=-4] a\n",
       "    di:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 di\n",
       "    dj:f32[] = integer_pow[y=-5] a\n",
       "    dk:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 dk\n",
       "    _:f32[] = mul -4.0 dj\n",
       "    dl:f32[] = integer_pow[y=-5] a\n",
       "    dm:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 dm\n",
       "    dn:f32[] = integer_pow[y=-6] a\n",
       "    do:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 do\n",
       "    _:f32[] = mul -5.0 dn\n",
       "    _:f32[] = mul -4.0 dl\n",
       "    _:f32[] = mul -3.0 dh\n",
       "    _:f32[] = mul -2.0 cz\n",
       "    dp:f32[] = integer_pow[y=-3] a\n",
       "    dq:f32[] = integer_pow[y=-4] a\n",
       "    _:f32[] = mul -3.0 dq\n",
       "    dr:f32[] = integer_pow[y=-4] a\n",
       "    ds:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 ds\n",
       "    _:f32[] = mul -3.0 dr\n",
       "    dt:f32[] = integer_pow[y=-4] a\n",
       "    du:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 du\n",
       "    dv:f32[] = integer_pow[y=-5] a\n",
       "    dw:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 dw\n",
       "    _:f32[] = mul -4.0 dv\n",
       "    _:f32[] = mul -3.0 dt\n",
       "    dx:f32[] = integer_pow[y=-4] a\n",
       "    dy:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 dy\n",
       "    dz:f32[] = integer_pow[y=-5] a\n",
       "    ea:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ea\n",
       "    _:f32[] = mul -4.0 dz\n",
       "    eb:f32[] = integer_pow[y=-5] a\n",
       "    ec:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ec\n",
       "    ed:f32[] = integer_pow[y=-6] a\n",
       "    ee:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 ee\n",
       "    _:f32[] = mul -5.0 ed\n",
       "    _:f32[] = mul -4.0 eb\n",
       "    _:f32[] = mul -3.0 dx\n",
       "    ef:f32[] = integer_pow[y=-4] a\n",
       "    eg:f32[] = integer_pow[y=-5] a\n",
       "    _:f32[] = mul -4.0 eg\n",
       "    eh:f32[] = integer_pow[y=-5] a\n",
       "    ei:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ei\n",
       "    _:f32[] = mul -4.0 eh\n",
       "    ej:f32[] = integer_pow[y=-5] a\n",
       "    ek:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 ek\n",
       "    el:f32[] = integer_pow[y=-6] a\n",
       "    em:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 em\n",
       "    _:f32[] = mul -5.0 el\n",
       "    _:f32[] = mul -4.0 ej\n",
       "    en:f32[] = integer_pow[y=-5] a\n",
       "    eo:f32[] = integer_pow[y=-6] a\n",
       "    _:f32[] = mul -5.0 eo\n",
       "    ep:f32[] = integer_pow[y=-6] a\n",
       "    eq:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 eq\n",
       "    _:f32[] = mul -5.0 ep\n",
       "    er:f32[] = integer_pow[y=-6] a\n",
       "    es:f32[] = integer_pow[y=-7] a\n",
       "    _:f32[] = mul -6.0 es\n",
       "    et:f32[] = integer_pow[y=-7] a\n",
       "    eu:f32[] = integer_pow[y=-8] a\n",
       "    ev:f32[] = mul -7.0 eu\n",
       "    ew:f32[] = mul -6.0 et\n",
       "    ex:f32[] = mul -5.0 er\n",
       "    ey:f32[] = mul -4.0 en\n",
       "    ez:f32[] = mul -3.0 ef\n",
       "    fa:f32[] = mul -2.0 dp\n",
       "    fb:f32[] = mul 1.0 cj\n",
       "    fc:f32[] = mul fb d\n",
       "    _:f32[] = neg fc\n",
       "    fd:f32[] = neg 1.0\n",
       "    fe:f32[] = mul fd d\n",
       "    ff:f32[] = mul 1.0 fe\n",
       "    _:f32[] = mul ff fa\n",
       "    fg:f32[] = mul ff 1.0\n",
       "    fh:f32[] = mul -2.0 fg\n",
       "    _:f32[] = mul fh ez\n",
       "    fi:f32[] = mul fh 1.0\n",
       "    fj:f32[] = mul -3.0 fi\n",
       "    _:f32[] = mul fj ey\n",
       "    fk:f32[] = mul fj 1.0\n",
       "    fl:f32[] = mul -4.0 fk\n",
       "    _:f32[] = mul fl ex\n",
       "    fm:f32[] = mul fl 1.0\n",
       "    fn:f32[] = mul -5.0 fm\n",
       "    _:f32[] = mul fn ew\n",
       "    fo:f32[] = mul fn 1.0\n",
       "    fp:f32[] = mul -6.0 fo\n",
       "    fq:f32[] = mul fp ev\n",
       "  in (fq,) }"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could go forever:\n",
    "make_jaxpr(jax.grad(jax.grad(jax.grad(jax.grad(jax.grad(jax.grad(jax.grad(jax.grad(f)))))))))(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[] b:f32[] c:f32[]. let\n",
       "    d:f32[] = mul b a\n",
       "    e:f32[] = add d c\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's another function: a layer of a neural network\n",
    "def mlp(x, w, b):\n",
    "    return jnp.dot(w, x) + b\n",
    "\n",
    "x = 9.0\n",
    "w = 3.0\n",
    "b = 1.0\n",
    "\n",
    "make_jaxpr(mlp)(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[] b:f32[] c:f32[]. let\n",
       "    d:f32[] = mul b a\n",
       "    _:f32[] = add d c\n",
       "    e:f32[] = mul 1.0 a\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the gradient of f with respect to w\n",
    "make_jaxpr(jax.grad(mlp, argnums=1))(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool, weak_type=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analytical gradient is just x\n",
    "x == jax.grad(mlp, argnums=1)(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[3,3] b:f32[3,3] c:f32[3,3] d:f32[3]. let\n",
       "    e:f32[3] = dot_general[dimension_numbers=(([1], [0]), ([], []))] b d\n",
       "    f:f32[3] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a d\n",
       "    g:f32[3] = dot_general[dimension_numbers=(([1], [0]), ([], []))] c d\n",
       "    h:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] e f\n",
       "    i:f32[] = sqrt 3.0\n",
       "    j:f32[] = convert_element_type[new_dtype=float32 weak_type=False] i\n",
       "    k:f32[] = div h j\n",
       "    l:f32[] = exp k\n",
       "    m:f32[] = exp k\n",
       "    n:f32[] = reduce_sum[axes=()] m\n",
       "    o:f32[] = div l n\n",
       "    p:f32[3] = mul o g\n",
       "  in (p,) }"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's get crazy: here's the attention mechanism from the Transformer\n",
    "# (https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "def softmax(x):\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v):\n",
    "    qk = jnp.dot(q, k.T) / jnp.sqrt(q.shape[-1])\n",
    "    weights = softmax(qk)\n",
    "    return jnp.dot(weights, v)\n",
    "\n",
    "def attention(weights, e):\n",
    "    Wq = weights[\"Wq\"]\n",
    "    Wk = weights[\"Wk\"]\n",
    "    Wv = weights[\"Wv\"]\n",
    "\n",
    "    q = jnp.dot(Wq, e)\n",
    "    k = jnp.dot(Wk, e)\n",
    "    v = jnp.dot(Wv, e)\n",
    "    return scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "Wq = jnp.array([[1.0, 0.1, 0.0],\n",
    "                [0.0, 1.0, 0.1],\n",
    "                [0.1, 0.0, 1.0]])\n",
    "\n",
    "Wk = jnp.array([[1.0, 0.0, 0.1],\n",
    "                [0.1, 1.0, 0.0],\n",
    "                [0.0, 0.1, 1.0]])\n",
    "\n",
    "Wv = jnp.array([[1.0, 0.1, 0.0],\n",
    "                [0.0, 1.0, 0.1],\n",
    "                [0.1, 0.0, 1.0]])\n",
    "\n",
    "\n",
    "e = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "make_jaxpr(attention)(dict(Wq=Wq, Wk=Wk, Wv=Wv), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1.2, 2.3, 3.1], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(dict(Wq=Wq, Wk=Wk, Wv=Wv), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Wk': Array([[[-8.2590617e-08, -1.6518123e-07, -2.4777185e-07],\n",
       "         [-1.5829868e-07, -3.1659735e-07, -4.7489601e-07],\n",
       "         [-2.1335909e-07, -4.2671817e-07, -6.4007725e-07]],\n",
       " \n",
       "        [[-1.6518123e-07, -3.3036247e-07, -4.9554370e-07],\n",
       "         [-3.1659735e-07, -6.3319470e-07, -9.4979202e-07],\n",
       "         [-4.2671817e-07, -8.5343635e-07, -1.2801545e-06]],\n",
       " \n",
       "        [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]]], dtype=float32),\n",
       " 'Wq': Array([[[-8.9473168e-08, -1.7894634e-07, -2.6841951e-07],\n",
       "         [-1.4453357e-07, -2.8906715e-07, -4.3360072e-07],\n",
       "         [-2.2024165e-07, -4.4048329e-07, -6.6072494e-07]],\n",
       " \n",
       "        [[-1.7894634e-07, -3.5789267e-07, -5.3683902e-07],\n",
       "         [-2.8906715e-07, -5.7813429e-07, -8.6720144e-07],\n",
       "         [-4.4048329e-07, -8.8096658e-07, -1.3214499e-06]],\n",
       " \n",
       "        [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]]], dtype=float32),\n",
       " 'Wv': Array([[[1., 2., 3.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0.],\n",
       "         [1., 2., 3.],\n",
       "         [0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 2., 3.]]], dtype=float32)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the gradient of the attention mechanism with respect to the weights\n",
    "# note that we have to use the full jacobian, since the output of our function\n",
    "# is a vector, not a scalar\n",
    "# the result is a 3x3 matrix, where each row is the gradient of the output\n",
    "# with respect to the corresponding row of the weight matrix\n",
    "# note how simple the Wv gradient is, since all we do is a dot product\n",
    "jax.jacobian(attention, argnums=0)(dict(Wq=Wq, Wk=Wk, Wv=Wv), e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vmap (automatic vectorization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have you ever written a very simple function that you wanted to apply to a large array of inputs? numpy makes this easy with vectorization, but jax takes it a step further with vmap.\n",
    "\n",
    "vmap is a function transformation that allows you to take a function that operates on a single input, and transform it into a function that operates on many inputs in a vectorized manner.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First example: zipping!\n",
    "\n",
    "in python, zip is a function that takes two lists and returns a list of pairs, operated on in-step:\n",
    "\n",
    "```python\n",
    "out = [f(x[i],y[i]) for zip(x,y)]\n",
    "```\n",
    "\n",
    "here's how you would do it in jax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([5, 7, 9], dtype=int32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func(x,y):\n",
    "    return x+y\n",
    "\n",
    "x_batched = jnp.array([1,2,3])\n",
    "y_batched = jnp.array([4,5,6])\n",
    "vmap_func = jax.vmap(func, in_axes = (0,0))\n",
    "vmap_func(x_batched,y_batched)  # [x1+y1, x2+y2, x3+y3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what about vectorising over non-leading axes?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t can happen that your batch dimension for the various arguments differ\n",
    "\n",
    "for example, we may want to perform the same sum above, but our axes are at different positions for the two arguments\n",
    "\n",
    "```python3\n",
    "x = [[1,2,3]] # shape (1,3)\n",
    "y = [[4],[5],[6] # shape (3,1)\n",
    "```\n",
    "In this case our Python code would look like\n",
    "\n",
    "```python3\n",
    "out = [f(x[0][i],x[i][0]) for i, (x,y) in enumerate(zip(x,y))]\n",
    "```\n",
    "JAX can handle this no problem!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[5, 7, 9]], dtype=int32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batched = jnp.array([[1,2,3]])\n",
    "y_batched = jnp.array([[4],[5],[6]])\n",
    "# here, we specify that the first argument is batched along axis 1, \n",
    "# and the second argument is batched along axis 0.\n",
    "# we also specify where we want the batched axis to be in the output\n",
    "out1 = jax.vmap(func, in_axes = (1,0), out_axes=1)(x_batched,y_batched)\n",
    "out1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "for example, consider the following function that would be a litte tricky to vectorize with numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, inputs):\n",
    "  for W, b in params:\n",
    "    outputs = jnp.dot(inputs, W) + b\n",
    "    inputs = jnp.tanh(outputs)\n",
    "  return outputs\n",
    "\n",
    "params = [\n",
    "    (jnp.array([[1., 2.], [3., 4.]]), jnp.array([1., 1.])),\n",
    "]  # params is a list of (weights, biases) arrays\n",
    "\n",
    "inputs = jnp.array([[1., 2.]])\n",
    "predict(params, inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we can use vmap to vectorize this function over multiple inputs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 8., 11.],\n",
       "       [16., 23.],\n",
       "       [24., 35.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_predict = jax.vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "many_inputs = jnp.array([[1., 2.], [3., 4.], [5., 6.]])\n",
    "batched_predict(params, many_inputs)  # shape: (batch_size, output_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a maximum of three brain cells, we have implemented batching! :)\n",
    "\n",
    "the `in_axes` argument specifies which of the functionâ€™s arguments to batch over. in this case, we want to batch over the second argument, which is the inputs. the first argument, params, will be left unchanged -- it will be as if we called predict in a for loop over each of the inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 8., 11.],\n",
       "        [16., 23.],\n",
       "        [24., 35.]],\n",
       "\n",
       "       [[ 8., 11.],\n",
       "        [16., 23.],\n",
       "        [24., 35.]]], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just like grad, vmap can be composed\n",
    "batch_of_batches = jnp.array([many_inputs, many_inputs])\n",
    "batched_batched_predict = jax.vmap(batched_predict, in_axes=(None, 0))\n",
    "batched_batched_predict(params, batch_of_batches) # shape: (batch, batch, output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's another one of my favorite examples -- operating over pairs of inputs using nested vmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 50), (50, 50))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_range = jnp.linspace(-5,5)\n",
    "y_range = jnp.linspace(-2,2)\n",
    "\n",
    "\n",
    "def func(x,y):\n",
    "    return jnp.sin(x)*y\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# this is the unbroadcasted version\n",
    "out_np = np.zeros((len(x_range),len(y_range)))\n",
    "for i,x in enumerate(x_range):\n",
    "    for j,y in enumerate(y_range):\n",
    "        out_np[i,j] = func(x,y)\n",
    "\n",
    "\n",
    "# maybe not the best example since it's actually completely vectorizable already\n",
    "x_grid, y_grid = np.meshgrid(x_range,y_range, indexing = 'ij')\n",
    "out_np = func(x_grid.T,y_grid.T)\n",
    "\n",
    "\n",
    "# but, if you only had the implementation for a single x and y, you could use vmap\n",
    "# start with the inner function, which batches over x and leaves y constant\n",
    "inner_func = jax.vmap(func, in_axes = (0,None))\n",
    "# then leave x constant, and batch over y\n",
    "outer_func = jax.vmap(inner_func, in_axes = (None,0))\n",
    "# the result is a function that works over the entire grid\n",
    "out = outer_func(x_range,y_range)\n",
    "\n",
    "# I like this because it lets me control the level of batching :)\n",
    "# we could do this for unlimited dimensions - just keep adding vmaps\n",
    "# for me thats great cause my head hurts when i try to do stuff with 6-D tensors in numpy...\n",
    "\n",
    "out_np.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1050x450 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "subplot_settings = dict(figsize=[7, 3], dpi=150, tight_layout=True)\n",
    "fig, axs = plt.subplots(1, 2, **subplot_settings)\n",
    "axs[0].imshow(out_np)\n",
    "axs[0].set_title(\"numpy\")\n",
    "axs[1].imshow(out)\n",
    "axs[1].set_title(\"jax\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jit (aka free speed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jit is a function transformation that will take a function and return a \"compiled\" version of it that will be optimized for the hardware you run it on. in almost all cases, using jit will dramatically improve the performance of your functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.4 Âµs Â± 1.25 Âµs per loop (mean Â± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# here's a complicated function that we want to speed up\n",
    "def complicated_function(x):\n",
    "    for _ in range(100):\n",
    "        x = np.sin(x)\n",
    "    for _ in range(100):\n",
    "        x = np.cos(x)\n",
    "    return x\n",
    "\n",
    "%timeit complicated_function(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653 Âµs Â± 2.44 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# here's the same function, but with jax\n",
    "def complicated_function(x):\n",
    "    for _ in range(100):\n",
    "        x = jnp.sin(x)\n",
    "    for _ in range(100):\n",
    "        x = jnp.cos(x)\n",
    "    return x\n",
    "\n",
    "%timeit complicated_function(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.7390851, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 times slower? what gives?\n",
    "# jax is doing other stuff like tracing as well as the numpy operation\n",
    "# we can get rid of that overhead by using jit and compiling the function\n",
    "\n",
    "@jax.jit  # this is the same as f = jax.jit(f)\n",
    "def complicated_function(x):\n",
    "    for _ in range(100):\n",
    "        x = jnp.sin(x)\n",
    "    for _ in range(100):\n",
    "        x = jnp.cos(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "complicated_function(1.0)  # run once to compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.56 Âµs Â± 36.2 ns per loop (mean Â± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# this is 100 times faster than numpy on my laptop!\n",
    "%timeit complicated_function(1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine neural network training: we do sooooo many forward passes and backward passes, and we want them to be as fast as possible. we can use jit to speed up our training loop, and compile all of those operations, optimised for the CPU/GPU/TPU we're running on. pretty cool right :) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 6.4318055e-21, -3.8199754e-21, -5.2034510e-19], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can arbitrarily nest jit, grad, and vmap in any order\n",
    "\n",
    "@jax.jit\n",
    "@jax.vmap\n",
    "@jax.grad\n",
    "def complicated_function(x):\n",
    "    for _ in range(100):\n",
    "        x = jnp.sin(x)\n",
    "    for _ in range(100):\n",
    "        x = jnp.cos(x)\n",
    "    return x\n",
    "\n",
    "complicated_function(jnp.array([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 6.4318055e-21, -3.8199754e-21, -5.2034510e-19], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same thing, but assignment style\n",
    "def complicated_function(x):\n",
    "    for _ in range(100):\n",
    "        x = jnp.sin(x)\n",
    "    for _ in range(100):\n",
    "        x = jnp.cos(x)\n",
    "    return x\n",
    "\n",
    "complicated_function = jax.jit(jax.vmap(jax.grad(complicated_function)))\n",
    "\n",
    "complicated_function(jnp.array([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
